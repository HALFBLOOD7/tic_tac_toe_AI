{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "6012aa208e5ce15976869f0fa23ec4dc1a4f62f1ded6b24f74ed4cea2f10b0f7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all nececarry modules\n",
    "from collections import deque\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining game environment\n",
    "class tic_tac_toe:\n",
    "    def __init__(self):\n",
    "        self.state = [0,0,0,0,0,0,0,0,0]\n",
    "        self.input_size = 9\n",
    "        self.output_size = 9\n",
    "        self.player1=True\n",
    "        self.move=0\n",
    "        self.last_move = 0\n",
    "\n",
    "    def done(self,filled):\n",
    "        if (filled[0] == filled[1]== filled[2]!=0):\n",
    "             return True\n",
    "        elif (filled[0] == filled[3]== filled[6]!=0):\n",
    "            return True\n",
    "        elif (filled[0] == filled[4]== filled[8]!=0):\n",
    "            return True\n",
    "        elif (filled[1] == filled[4]== filled[7]!=0):\n",
    "            return True\n",
    "        elif (filled[2] == filled[4]== filled[6]!=0):\n",
    "            return True\n",
    "        elif (filled[2] == filled[5]== filled[8]!=0):\n",
    "            return True\n",
    "        elif (filled[3] == filled[4]== filled[5]!=0):\n",
    "            return True\n",
    "        elif (filled[6] == filled[7]== filled[8]!=0):\n",
    "            return True\n",
    "        elif self.move == 9:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [0,0,0,0,0,0,0,0,0]\n",
    "        self.player1=True\n",
    "        self.move=0\n",
    "        self.last_move = 0\n",
    "\n",
    "    def validate_move(self,move):\n",
    "        if(0<=move<10):\n",
    "            return(True) if (self.state[move]==0) else False\n",
    "        return(False)\n",
    "\n",
    "\n",
    "    def cell_move(self,move):\n",
    "        if(self.validate_move(move)):\n",
    "            self.move = self.move + 1\n",
    "            self.state[move] = 1 if(self.player1) else 2\n",
    "            self.player1 = not self.player1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def step(self,move):\n",
    "        state= np.copy(self.state)\n",
    "        reward = self.reward(state,move)\n",
    "        self.move = self.move+1\n",
    "        self.last_move = move+1\n",
    "        self.state[move] = 1 if(self.player1) else 2\n",
    "        \n",
    "        next_state = self.state\n",
    "        done = self.done(next_state)\n",
    "        self.player1 = not self.player1\n",
    "        return reward,next_state,done\n",
    "\n",
    "\n",
    "    def reward(self,state,move):\n",
    "        local_state = np.copy(state)\n",
    "        local_state[move] = 1 if(self.player1) else 2\n",
    "        if(self.done(local_state)):\n",
    "            return 40\n",
    "        reward = self.missed_win(state,move) + self.accepted_loss(state,move) + self.blocked_win(state,move)\n",
    "        return reward\n",
    "    \n",
    "    def missed_win(self,state,move):\n",
    "        local_copy = np.copy(state)\n",
    "        for cell in range(9):\n",
    "            if(local_copy[cell] == 0 and not(cell == move) ):\n",
    "                local_copy[cell] = 1 if(self.player1) else 2\n",
    "                if(self.done(local_copy)):\n",
    "                    local_copy[cell] = 0 \n",
    "                    print(\"missed win -20\",move,\"\\n\",np.reshape(local_copy,(3,3)))\n",
    "                    return -20\n",
    "                else:\n",
    "                    local_copy[cell] = 0\n",
    "        return 0\n",
    "\n",
    "    def accepted_loss(self,state,move):\n",
    "        local_copy = np.copy(state)\n",
    "        for cell in range(9):\n",
    "            if(local_copy[cell]==0 and not(cell == move) ):\n",
    "                local_copy[cell] = 1 if(not self.player1) else 2\n",
    "                if(self.done(local_copy)):\n",
    "                    local_copy[cell] = 0 \n",
    "                    print(\"accepted loss -20\",move,\"\\n\",np.reshape(local_copy,(3,3)))\n",
    "                    return -20\n",
    "                else:\n",
    "                    local_copy[cell] = 0\n",
    "        return 0\n",
    "\n",
    "    def blocked_win(self,state,move):\n",
    "        local_copy = np.copy(state)\n",
    "        local_copy[move] = 1 if(not self.player1) else 2\n",
    "        if(self.done(local_copy)):\n",
    "            local_copy[move] = 0 \n",
    "            print(\"blocked win 20\",move,\"\\n\",np.reshape(local_copy,(3,3)))\n",
    "            return 20\n",
    "        return 0\n",
    "\n",
    "    def print_game_state(self):\n",
    "        print(\"player2\" if self.player1 else \"player1\",self.last_move)\n",
    "        print(np.reshape(np.copy(self.state),(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining agent\n",
    "class agent:\n",
    "    def __init__(self,active_game):\n",
    "        self.active_game=active_game\n",
    "\n",
    "        self.input_size = self.active_game.input_size\n",
    "        self.output_size=self.active_game.output_size\n",
    "        \n",
    "        self.memory = deque(maxlen = 100)\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = self.create_model()\n",
    "    \n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(22,input_dim=self.active_game.input_size,activation='relu'))\n",
    "        model.add(Dense(11,activation='relu'))\n",
    "        model.add(Dense(self.active_game.output_size,activation='softmax'))\n",
    "\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def play(self,i):\n",
    "        return(self.active_game.cell_move(i))\n",
    "    \n",
    "    def act(self):\n",
    "        local_copy = np.copy(self.active_game.state)\n",
    "        allowed_moves = np.where(local_copy==0)[0]\n",
    "        \n",
    "        if np.random.rand()<self.epsilon:\n",
    "            return np.random.choice(allowed_moves,1)[0]\n",
    "        \n",
    "        local_copy = np.reshape(np.copy(local_copy),(1,9))\n",
    "        prediction = self.model.predict(local_copy)[0]\n",
    "        predicted_action = np.where(prediction == np.amax(prediction))[0][0]\n",
    "        \n",
    "        \n",
    "        return  predicted_action\n",
    "\n",
    "\n",
    "    def remember(self,player1,state,action,reward,next_state,done):\n",
    "        self.memory.append((player1,state,action,reward,next_state,done))\n",
    "    \n",
    "    def short_train(self,state,action,reward):\n",
    "        local_state = np.reshape(np.copy(state),(1,9))\n",
    "\n",
    "        target_f = np.array(self.model.predict(local_state))\n",
    "        target_f[0][action] = reward\n",
    "\n",
    "        self.model.fit(local_state,target_f,epochs=1,verbose=0)\n",
    "\n",
    "    def train(self,batch_size=15):\n",
    "        mini_batch = random.sample(self.memory,batch_size)\n",
    "\n",
    "        for exprience in mini_batch:\n",
    "            player1,state,action,reward,next_state,done = exprience\n",
    "\n",
    "            local_state = np.reshape(np.copy(next_state),(1,9))\n",
    "\n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(local_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            \n",
    "            local_state = np.reshape(np.copy(state),(1,9))\n",
    "\n",
    "            target_f = np.array(self.model.predict(local_state))\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(local_state,target_f,epochs=1,verbose=0)\n",
    "\n",
    "        if(self.epsilon>self.epsilon_min):\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "active_game = tic_tac_toe()\n",
    "player=agent(active_game)\n",
    "batch_size=15\n",
    "outputdir_ = \"tic_tac_toe_model\"\n",
    "\n",
    "play=True\n",
    "\n",
    "for episodes in range(10):\n",
    "    active_game.reset()\n",
    "    print(episodes,'episode')\n",
    "    while play:\n",
    "\n",
    "        state = active_game.state\n",
    "\n",
    "        while True:\n",
    "            action = player.act()\n",
    "            if(active_game.validate_move(action)):\n",
    "                break\n",
    "            else:\n",
    "                next_state = np.copy(state)\n",
    "                next_state[action] = 1 if(active_game.player1) else 2\n",
    "                player.remember(active_game.player1,state,action,-100,next_state,False)\n",
    "                player.short_train(state,action,-100)\n",
    "\n",
    "  \n",
    "        reward,next_state,done =  active_game.step(action)\n",
    "\n",
    "        player.remember(active_game.player1,state,action,reward,next_state,done)\n",
    "        \n",
    "        if(episodes>8):\n",
    "            print(\"move\",active_game.move)\n",
    "            print(active_game.print_game_state())\n",
    "            print(\"------------------------\")\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            if len(player.memory) > batch_size:\n",
    "                player.train(batch_size)\n",
    "            if (episodes+1) % 10 == 0:\n",
    "                player.save(outputdir_+\"weights\"+'{:04d}'.format(episodes+1)+\".hdf5\")\n",
    "            print(\"player2\") if(active_game.player1) else print(\"draw\") if (active_game.move == 9 and not active_game.done(next_state)) else print(\"player1\")\n",
    "            break\n",
    "    print('') \n",
    "            \n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing game with trained model\n",
    "# calling certain episode number trained model\n",
    "episode_number = 10\n",
    "active_game = tic_tac_toe()\n",
    "player = agent(active_game)\n",
    "player.load(\"tic_tac_toe_modelweights\"+'{:04d}'.format(episode_number)+\".hdf5\")\n",
    "print(player.model)\n",
    "\n",
    "# specific state for which the trained model is to be checked\n",
    "state = [[1,0,1],\n",
    "         [2,2,1],\n",
    "         [2,1,2]]\n",
    "local_copy = np.reshape(state,(1,9))\n",
    "print(local_copy)\n",
    "prediction = player.model.predict(local_copy)[0]\n",
    "predicted_action = np.where(prediction == np.amax(prediction))[0][0]\n",
    "print(prediction,predicted_action)"
   ]
  }
 ]
}
